{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsaIeAtF6T1G"
   },
   "source": [
    "## Librerías: envoltorios (wrappers)\n",
    "\n",
    "Los *wrappers* son un conjunto de librerías y herramientas (en otro lenguaje diferente a *C/C++*) que actúa como puente y oculta muchos de los detalles, funcionan como cajas negras. Varios lenguajes de progranación permiten usar estos *wrappers*, entre otros; *JAVA*, *Python*, *R*, etc.\n",
    "\n",
    "Los envoltorios más populares para *Python* son *Numba* y *TensorFlow*.\n",
    "\n",
    "\n",
    "Los 2 diferentes enfoques que se les puede dar a los envoltorios.\n",
    "\n",
    "<center>\n",
    "<img src=\"../Images/arribabajo.png\" width=\"600\"> \n",
    "</center>\n",
    "\n",
    "Flujo de *Numba*, ¿cómo es que se optimiza el código?.\n",
    "\n",
    "<center>\n",
    "<img src=\"../Images/numba.png\" width=\"600\"> \n",
    "</center>\n",
    "\n",
    "    \n",
    "\n",
    "Relación entre *TensorFlow* y *Nvidia*.\n",
    "\n",
    "<center>\n",
    "<img src=\"../Images/tensor2.png\" width=\"400\"> \n",
    "</center>\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"../Images/tensor1.png\" width=\"600\"> \n",
    "</center>\n",
    "\n",
    "Capas en el desarrollo de *software*.\n",
    "\n",
    "<center>\n",
    "<img src=\"../Images/tensordevice.jpg\" width=\"400\"> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PPn3oSp8WaM"
   },
   "source": [
    "# *Numba*\n",
    "\n",
    "*Numba* es uno envoltorio con un funcionamiento sencillo y fácil instalación.\n",
    "\n",
    "Optimizar el código logrand un mejor desempeñode distintas formas, algunas de ellas son las siguientes:\n",
    "\n",
    "• Convierte código *Python* en código de máquina: al compilar código empleando *Numba*, este convierte el código en código de máquina y la segunda vez que sea ejecutado este se ejecuta en lenguaje de muy bajo nivel que se traduce en una ejecución más rápida.\n",
    "\n",
    "• Es posible utilizar una capa (*layer*) para acceder a características de *OpenMP*.\n",
    "\n",
    "• Es posible paralelizar código empleando utilidades de *MPI*.\n",
    "\n",
    "• Tiene soporte para el uso de *GPU's* utilizando *CUDA* como *background*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PehjbuHF8sKR"
   },
   "source": [
    "## Ventajas\n",
    "\n",
    "*Numba* posee múltiples ventajas, aunque una de las más importantes es poder decidir como optimizar el código escrito en *Python*.\n",
    "\n",
    "Es muy sencillo de instalar mediante *pip* e igual de fácil de usar que *Python*.\n",
    "\n",
    "Se tiene una gran capacidad de acoplamiento con *Numpy* (biblioteca para cómputo científico).\n",
    "\n",
    "Además de ser posible optar por un mecanismo para optimizar el código, *Numba* permite escribir código híbrido que combine lo mejor de las diferentes formás de optimizar el desempeño.\n",
    "\n",
    "Emplear *Numba* es tan sencillo como importar la biblioteca y hacer uso de sus **decoradores** para optimizar el código.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJhW1gUa9DdF"
   },
   "source": [
    "## Desventajas\n",
    "\n",
    "*Numba* tiene en realidad muy pocas desventajas.\n",
    "\n",
    "La más evidente de estas es que **encapsula mucho de su funcionamiento**, es decir que en realidad funciona como caja negra.\n",
    "\n",
    "Es importante reconocer si numba nos ayudara a optimizar el código, no mejorará el tiempo en todos los casos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rg_-5MXa94pG"
   },
   "source": [
    "### Decoradores\n",
    "\n",
    "Un decorador en *Numba* es una forma de modificar funciones de manera tal que pueda ser optimizada empleando alguna de las técnicas previamente mencionadas.\n",
    "\n",
    "Se puede pensar en un decorador en una función que recibe una función como parámetro y devuelve otra función optimizada como salida.\n",
    "\n",
    "Una función de *Python* es envuelta por uno o más decoradores, una vez que se define esta función el decorador es evaluado y *Numba* devuelve una función optimizada que puede ser invocada desde *Python*.\n",
    "\n",
    "El alcance del ó de los decoradores se limita al alcance de la función definida a la cual se le aplique dichos decoradores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QY9qCM04-zEJ"
   },
   "source": [
    "\n",
    "`nopython`: se utiliza mediante la sentencia `@jit(parametros)`.\n",
    "\n",
    "*Numba* se puede llamar con el decorador `@jit(nopython=True)`.\n",
    "\n",
    "Esta sentencia lo que le indica a *Numba* es que el código en el cual esta envuelta la función, debe ser compilado y ejecutado sin utilizar el entono de *Python*. Lo que significa que una vez que ha sido compilada esta función se ejecutara de manera más eficiente que empleando el interprete de *Python*. \n",
    "\n",
    "Otro modo de compilación es *object mode*, y se utiliza de la siguiente forma: `nopython=True`, es decir `@jit` sin parámetros. Sin embargo este modo se limita a optimizar unicamente los ciclos y no todo el código definido en la función.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0LL39Jv-9l8"
   },
   "source": [
    "`parallel` se utiliza para ejecutar ciclos en paralelo, se llama como sigue `@njit(parrallel=True)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uifxfic__Wcw"
   },
   "source": [
    "## *Numba* + *CUDA*\n",
    "\n",
    "*Numba* ofrece soporte para programación de *GPU* mediante *CUDA*, permitiendo compilar un subconjunto restringido de código escrito en *Python* que se traduce en funciones tipo *Kernel* y tipo *Device*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asKyxjQD9Pip"
   },
   "source": [
    "## Instalación en equipo local\n",
    "\n",
    "Dado que a partir de este punto del curso es posible emplear diferentes versiones de *Python* o de sus bibliotecas, se recomienda crear un **entorno virtual** y en este entorno instalar *numba*.\n",
    "\n",
    "Supongamos que ya se cuenta *virtualenv*, *Python* 2.7 y *pip*.\n",
    "\n",
    "1.   Crear entorno virtual: \n",
    "\n",
    "`\\$mkdir numba`\n",
    "\n",
    "`\\$virtualenv numba`\n",
    "\n",
    "2.   Activar entorno virtual:\n",
    "\n",
    "`\\$source numba/bin/activate`\n",
    "\n",
    "3.   Instalar *Numba*:\n",
    "\n",
    "`(numba)\\$pip install numba`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geIA0krQ2-VZ"
   },
   "source": [
    "## *Numba* en *Google Colab*\n",
    "\n",
    "Para utilizar *Numba* en *Google Colab*, es tan sencillo como importar la biblioteca y utilizar sus decoradores, a continuación se muestra el ejemplo de la aproximación de $pi$ optimizado mediante *Numba*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "brKZ9XmT3VnF",
    "outputId": "fb0190bf-f700-4dd2-8801-693c141303d2"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from timeit import default_timer as timer\n",
    "# biblioteca de Numba\n",
    "from numba import jit\n",
    "\n",
    "#¡¡¡descomentar para optimizar, te vas a sorprender!!!\n",
    "#@jit(nopython=True)\n",
    "def mc_pi_aprox(n=100000000):\n",
    "    dentro_circulo = 0 \n",
    "    for i in range(n):\n",
    "      x = random.random()\n",
    "      y = random.random()\n",
    "      # valores dentro de la circunferencia\n",
    "      if (x**2+y**2 < 1):\n",
    "         dentro_circulo += 1\n",
    "    return 4*dentro_circulo / n\n",
    "\n",
    "# inicial\n",
    "inicio = timer()\n",
    "# algoritmo\n",
    "print(mc_pi_aprox())\n",
    "# final\n",
    "final = timer()\n",
    "\n",
    "# tiempo\n",
    "print('Tomo:',final - inicio, 'segundos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stEwY6wIBNBZ"
   },
   "source": [
    "# *TensorFlow*\n",
    "\n",
    "*TensorFlow* es un conjunto de herramientas que proporciona *Google* para el desarrollo de algoritmos de aprendizaje automático, algunas de sus características son:\n",
    "\n",
    "*   Cuenta con diferentes versiones de su *API* para lenguajes tales como: *C++, Haskell, Java* y *Go* entre algunos, aunque la versión más usada es la de *Python*.\n",
    "*   Existen versiones optimizadas de *TensorFlow* que hacen uso de programación mediante *GPU's* o incluso mediante *TPU's*.\n",
    "*   Una de sus aplicaciones más comunes es en el desarrollo de **redes neuronales e inteligencia artificial**.\n",
    "*   El desarrollo de *TensorFlow* es mediante licencia de código abierto, lo que significa que no hace falta pagar una licencia para hacer uso del mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlEvJkJPBUtk"
   },
   "source": [
    "## Aplicaciones\n",
    "\n",
    "*TensorFlow* tiene bastas aplicaciones, algunas de ellas son:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*   I.A. detrás de las fotografiás en *smarthphones*: recientemente se ha empleado técnicas de i.a. para mejorar la captura de imágenes en un *smartphone*, detrás de esta i.a podemos encontrar bibliotecas como *TensorfFlow*.\n",
    "*   Diagnostico médico: *TensorFlow* ya está mejorando las herramientas que utilizan los médicos, por ejemplo ayudando a analizar radiografías o fotografiás de pacientes y sugiriendo un diagnostico casi de inmediato.\n",
    "*   Procesamiento de imágenes: una de las aplicaciones más conocidas de *TensorFlow* es el *software* automatizado de procesamiento de imágenes, ***DeepDream*** es de los ejemplos mas conocidos al respecto. \n",
    "*   El desarrollo de *TensorFlow* es mediante licencia de código abierto, lo que significa que no hace falta pagar una licencia para hacer uso del mismo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sX6YRj3mBavU"
   },
   "source": [
    "## ¿Cómo funciona?\n",
    "\n",
    "La idea de *TensorFlow 1.x* es definir el cómputo de datos como una gráfica conformada por **tensores** (nodos) y **datos** (aristas).\n",
    "\n",
    "*   **Grafo**: un grafo es un conjunto de nodos y aristas que representan el cómputo de información que se recibe como entrada.\n",
    "*   **Nodo**: es una agrupación de datos que puede tomar diferentes formas dependiendo del rango; rango 0 es un escalar, rango 1 es un vector, rango 2 una matriz, etc.\n",
    "*   **Tensor**: en el contexto de *TensorFlow*, un tensor es conocido como una operación (op), la cual recibe uno o más datos o incluso la salida de otro tensor y realiza la operación indicada.\n",
    "*   **Sesión**: para poder realizar los cálculos definidos en el grafo, se debe llevar a cabo mediante una sesión que representa el cálculo que se desea realizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgF-TNuVBjec"
   },
   "source": [
    "##Visualización\n",
    "\n",
    "Podemos pensar en un grafo de la siguiente forma.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/op1.png?raw=1\" width=\"450\"> \n",
    "</center>\n",
    "\n",
    "Un perceptrón simple se ve así.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/op2.png?raw=1\" width=\"600\"> \n",
    "</center>\n",
    "\n",
    "Una capa de una red neuronal se ve de esta manera.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/opn.png?raw=1\" width=\"600\"> \n",
    "</center>\n",
    "\n",
    "Finalmente una red neuronal (*SOM*), la podemos pensar así.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/som.gif?raw=1\" width=\"600\"> \n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6qgBqlwCGJO"
   },
   "source": [
    "##  Instalación en equipo local\n",
    "\n",
    "Supongamos que ya se cuenta *virtualenv, python 2.7 y pip*.\n",
    "\n",
    "1.   Crear entorno virtual: \n",
    "\n",
    "`\\$mkdir tensorflow`\n",
    "\n",
    "`\\$virtualenv tensorflow`\n",
    "\n",
    "2.   Activar entorno virtual:\n",
    "\n",
    "`\\$source tensorflow/bin/activate`\n",
    "\n",
    "3.   Instalar *tensorflow*:\n",
    "\n",
    "`(tensorflow)\\$pip install tensorflow==1.0`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBIJYwmg48Zr"
   },
   "source": [
    "## *Tensorflow* 1.x en *Google Colab*\n",
    "\n",
    "En caso de usar la versión más reciente de *TensorFlow*, sucede igual que con *Numba*, solo es necesario importar la biblioteca y hacer uso de la misma, en otro caso es necesario desintalar la versión actual e instalar la versión necesaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YYYouZoXxuP0",
    "outputId": "c84d6345-9be3-4a45-9be0-a361d1210076"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24660/2929175791.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#%tensorflow_version 1.x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# importamos tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# se genera una operacion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# se le indica a python la version de tensorflow\n",
    "#%tensorflow_version 1.x\n",
    "# importamos tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# se genera una operacion\n",
    "a = tf.add(3, 5)\n",
    "# se muestran las caracteristicas de esta operacion\n",
    "print(a)\n",
    "\n",
    "# es solo hasta el momento de iniciar una sesion que se puede ver un resultado\n",
    "sess = tf.Session()\n",
    "# esta forma de definir el flujo de datos esta relacionado con ml\n",
    "print(sess.run(a))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9fB90WK51MJ"
   },
   "source": [
    "# Otros envoltorios\n",
    "\n",
    "Existe una gran variedad de envoltorios para multiples lenguajes, por ejemplo la clase *Thread* o la interfaz *Runnable* del lenguaje *Java*.\n",
    "\n",
    "Un ejemplo de lo que se puede realizar con este tipo de envoltorios es el entorno de [*NetLogo*](http://www.netlogoweb.org/launch#http://www.netlogoweb.org/assets/modelslib/Sample%20Models/Art/Follower.nlogo), herramienta/lenguaje de simulación de modelos basados en agentes.\n",
    "\n",
    "En *Python* existe una cantidad enorme de envoltorios como: *PyThorch, Keras, PyCuda*, etc. De igual manera para otros lenguajes existen sus respectivas versiones de envoltorios que tienen sus fundamentos en *OpenMP, MPI* y *CUDA*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dsaFmbyCamn"
   },
   "source": [
    "# Glosario\n",
    "\n",
    "*Layer*: Capa informática, nivel o capa que se oculta una parte del *software*.\n",
    "\n",
    "*Background*: En computación entorno que da soporte a un determinado software. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3J_aD2oaChF-"
   },
   "source": [
    "# Referencias\n",
    "\n",
    "1. https://numba.pydata.org/numba-doc/latest/user/5minguide.html\n",
    "\n",
    "2. https://nyu-cds.github.io/python-numba/01-jit/\n",
    "\n",
    "3. https://christophdeil.com/download/2019-07-11_Christoph_Deil_Numba.pdf\n",
    "\n",
    "4. https://numba.pydata.org/numba-doc/dev/cuda/kernels.html\n",
    "\n",
    "5. Tolga Soyata:\\newblock GPU Parallel Program Development Using CUDA."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Envoltorios_SCP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
